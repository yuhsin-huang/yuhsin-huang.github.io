---
layout: post
title: Blog Post 5 - Image Classification
---

In this blog post, I will perform image classification that classfies dogs and cats in Tensorflow.<br>
I will use *Tensorflow Datasets* to organize operations on training, validation, and test data sets. Then, by utilizing techniques such as *data augmentation*, *data preprocessing*, and *transfer learning* while modeling, I will create an image classification algorithm with over 95% accuracy score.

## ยง1. Load Packages and Obtain Data
**As usual, we import packages and acquire data first.**

```
import os
import tensorflow as tf
from tensorflow.keras import utils, layers, applications
import matplotlib.pyplot as plt
import random
```

**Here is the code to obtain our data:**

```
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```
The above code block creates TensorFlow Datasets for training, validation, and testing. 
We utilize a keras utility function called image_dataset_from_directory to construct a Dataset. In this function, we specify where the images are located, the randomness of the data, the batch size of the data, and the image size of the data.
<br>
**Next, we read the data:**

```
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

**To better understand our dataset, we create a function that creates a two_row visualization. In the first row, it will show three random pictures of cats, and in the second row, it will show three random pictures of dogs.**

```
def visualization(ds):

  # figure size
  plt.figure(figsize=(10, 10))
  
  #retrieve one batch (32 images with labels) from the data
  for images, labels in ds.take(1):
  
    # create two lists to contain cats and dogs
    C = []
    D = []

    for i in range(len(images)):
      # cats are labeled as 0
      if labels[i] == 0:
        C.append(i)
      # dogs are labeled as 1
      else:
        D.append(i)
    
    # randomly choose 3 samples from cats and dogs
    randC = random.sample(C, k=3)
    randD = random.sample(D, k=3)

    # create a 2x3 grids
    for i in range(6):
      ax = plt.subplot(2, 3, i + 1)

      # first row
      if i <= 2:
        plt.imshow(images[C[i]].numpy().astype("uint8"))
        plt.title('Cat')
        plt.axis("off")
        
      # second row
      else:
        plt.imshow(images[D[i-3]].numpy().astype("uint8"))
        plt.title('Dog')
        plt.axis("off")

visualization(train_dataset)
```
![post5.1.png](/images/post5.1.png)

**Now we check label frequencies and guess the accuracy of the *baseline machine learning model* based on our results.**

```
# create an iterator 
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()

# count numbers of cats and dogs
cat = 0
dog = 0

for i in labels_iterator:
    if i == 0:
      cat += 1
    else:
      dog += 1
```
After running this code block, we get that there are 1000 dog images and 1000 cat images in our dataset.
Since the *baseline machine learning model* always guesses the most frequent label, if we run this dataset under the *baseline machine learning model*, the accuracy score will then be 50%.

## ยง2. First Model
**In this section, we create our first tf.keras.Sequential model using layers such as Conv2D layers, MaxPooling2D layers, Flatten layer, Dense layer, and Dropout layer. Then we train our model and plot the history of the accuracy on both the training and validation sets.**

```
model1 = tf.keras.models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(.2)
])

# compile model
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train model
history = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
```
    Epoch 1/20
    63/63 [==============================] - 8s 94ms/step - loss: 45.3341 - accuracy: 0.4840 - val_loss: 1.8679 - val_accuracy: 0.5520
    Epoch 2/20
    63/63 [==============================] - 6s 85ms/step - loss: 2.7197 - accuracy: 0.5995 - val_loss: 2.3644 - val_accuracy: 0.5693
    Epoch 3/20
    63/63 [==============================] - 6s 85ms/step - loss: 2.8217 - accuracy: 0.6400 - val_loss: 1.9218 - val_accuracy: 0.5916
    Epoch 4/20
    63/63 [==============================] - 6s 85ms/step - loss: 2.1077 - accuracy: 0.6885 - val_loss: 2.0206 - val_accuracy: 0.5656
    Epoch 5/20
    63/63 [==============================] - 6s 84ms/step - loss: 1.8813 - accuracy: 0.7315 - val_loss: 2.6147 - val_accuracy: 0.5891
    Epoch 6/20
    63/63 [==============================] - 7s 102ms/step - loss: 1.9436 - accuracy: 0.7310 - val_loss: 2.4462 - val_accuracy: 0.5446
    Epoch 7/20
    63/63 [==============================] - 6s 87ms/step - loss: 1.7009 - accuracy: 0.7680 - val_loss: 2.8647 - val_accuracy: 0.5743
    Epoch 8/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.6645 - accuracy: 0.7815 - val_loss: 3.0603 - val_accuracy: 0.5668
    Epoch 9/20
    63/63 [==============================] - 6s 87ms/step - loss: 1.5301 - accuracy: 0.7975 - val_loss: 3.0685 - val_accuracy: 0.5817
    Epoch 10/20
    63/63 [==============================] - 6s 87ms/step - loss: 1.4534 - accuracy: 0.8145 - val_loss: 3.2552 - val_accuracy: 0.5705
    Epoch 11/20
    63/63 [==============================] - 7s 100ms/step - loss: 1.4535 - accuracy: 0.8150 - val_loss: 3.6655 - val_accuracy: 0.5545
    Epoch 12/20
    63/63 [==============================] - 6s 87ms/step - loss: 1.4080 - accuracy: 0.8310 - val_loss: 3.8696 - val_accuracy: 0.5606
    Epoch 13/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.2481 - accuracy: 0.8400 - val_loss: 3.7430 - val_accuracy: 0.5594
    Epoch 14/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.2536 - accuracy: 0.8440 - val_loss: 3.9284 - val_accuracy: 0.5594
    Epoch 15/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.2297 - accuracy: 0.8515 - val_loss: 3.9279 - val_accuracy: 0.5668
    Epoch 16/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.1958 - accuracy: 0.8750 - val_loss: 4.5159 - val_accuracy: 0.5656
    Epoch 17/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.1833 - accuracy: 0.8610 - val_loss: 5.4516 - val_accuracy: 0.5557
    Epoch 18/20
    63/63 [==============================] - 7s 104ms/step - loss: 1.1964 - accuracy: 0.8635 - val_loss: 4.9031 - val_accuracy: 0.5644
    Epoch 19/20
    63/63 [==============================] - 6s 85ms/step - loss: 1.2502 - accuracy: 0.8560 - val_loss: 4.4926 - val_accuracy: 0.5705
    Epoch 20/20
    63/63 [==============================] - 6s 87ms/step - loss: 1.1784 - accuracy: 0.8615 - val_loss: 4.9271 - val_accuracy: 0.5730

```

The validation accuracy of model1 stabilized between **54% and 59%** during training.
This result is a bit better than the baseline machine learning model (50% accuracy).
We also observe that the training accuracy is much higher than the validation accuracy, and hence there is an overfitting issue in model1.


## ยง3. Model with Data Augmentation
**To imporve our model's accuracy, we are going to add some data augmentation layers. Data augmentation layers will include modified copies of the same image in the training set. In this way, our model will have more data to learn *invariant features* of our input images.**

1. Create a tf.keras.layers.RandomFlip() layer that randomly flip the iamge horizontally and vertically.

```
first_layer = layers.RandomFlip()

# visualize how RandomFlip works
for images, labels in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  
  # images that has been flipped once
  next = first_layer(images[0])

  for i in range(4):
    ax = plt.subplot(1, 4, i + 1)
    
    # original image
    if i == 0:
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title("Original")
      plt.axis("off")
      
    else:
      plt.imshow(next.numpy().astype("uint8"))
      plt.title(i)
      plt.axis("off")
      
      # keep flipping the image
      next = first_layer(next)
```
![post5.2.png](/images/post5.2.png)

2. Create a tf.keras.layers.RandomRotation() layer that randomly rotate the image in a specified angle range.

```
# factor specifies the rotation angle range
second_layer = layers.RandomRotation(factor=(-0.2, 0.3), fill_mode = 'reflect')

# visualize how RandomRotation works
for images, labels in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  
  # images that has been rotated once
  next = second_layer(images[0])
  
  for i in range(4):
    ax = plt.subplot(1, 4, i + 1)
    
    # original image
    if i == 0:
      plt.imshow(images[i].numpy().astype("uint8"))
      plt.title("Original")
      plt.axis("off")
      
    else:
      plt.imshow(next.numpy().astype("uint8"))
      plt.title(i)
      plt.axis("off")
      # keep rotating the image
      next = second_layer(next)
```
![post5.3.png](/images/post5.3.png)

**Add these two data augmentation layers into our model1.**
```
model2 = tf.keras.models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    first_layer,
    second_layer,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(.2)
])

# compile
model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train
history = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
```
    Epoch 1/20
    63/63 [==============================] - 7s 93ms/step - loss: 2.4805 - accuracy: 0.5550 - val_loss: 1.2642 - val_accuracy: 0.6015
    Epoch 2/20
    63/63 [==============================] - 6s 91ms/step - loss: 2.1661 - accuracy: 0.5655 - val_loss: 2.1588 - val_accuracy: 0.5916
    Epoch 3/20
    63/63 [==============================] - 6s 92ms/step - loss: 2.2098 - accuracy: 0.5785 - val_loss: 1.3162 - val_accuracy: 0.5743
    Epoch 4/20
    63/63 [==============================] - 6s 91ms/step - loss: 2.1957 - accuracy: 0.5590 - val_loss: 1.3810 - val_accuracy: 0.5842
    Epoch 5/20
    63/63 [==============================] - 6s 91ms/step - loss: 2.1155 - accuracy: 0.5620 - val_loss: 1.0994 - val_accuracy: 0.6015
    Epoch 6/20
    63/63 [==============================] - 6s 91ms/step - loss: 2.1083 - accuracy: 0.5735 - val_loss: 1.1849 - val_accuracy: 0.6139
    Epoch 7/20
    63/63 [==============================] - 6s 90ms/step - loss: 2.1130 - accuracy: 0.5745 - val_loss: 1.5845 - val_accuracy: 0.6188
    Epoch 8/20
    63/63 [==============================] - 6s 92ms/step - loss: 2.0848 - accuracy: 0.5910 - val_loss: 1.3831 - val_accuracy: 0.6176
    Epoch 9/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.9699 - accuracy: 0.6015 - val_loss: 1.1953 - val_accuracy: 0.6337
    Epoch 10/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.9218 - accuracy: 0.6155 - val_loss: 1.3454 - val_accuracy: 0.6485
    Epoch 11/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8655 - accuracy: 0.6360 - val_loss: 1.1725 - val_accuracy: 0.6176
    Epoch 12/20
    63/63 [==============================] - 6s 91ms/step - loss: 2.0491 - accuracy: 0.5835 - val_loss: 1.3918 - val_accuracy: 0.6448
    Epoch 13/20
    63/63 [==============================] - 6s 90ms/step - loss: 1.9233 - accuracy: 0.6130 - val_loss: 1.2740 - val_accuracy: 0.6448
    Epoch 14/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8712 - accuracy: 0.6290 - val_loss: 1.1028 - val_accuracy: 0.6213
    Epoch 15/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8901 - accuracy: 0.6210 - val_loss: 1.1030 - val_accuracy: 0.6361
    Epoch 16/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8851 - accuracy: 0.6320 - val_loss: 1.1582 - val_accuracy: 0.6436
    Epoch 17/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8781 - accuracy: 0.6320 - val_loss: 1.2745 - val_accuracy: 0.6522
    Epoch 18/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8888 - accuracy: 0.6410 - val_loss: 1.2893 - val_accuracy: 0.6002
    Epoch 19/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8046 - accuracy: 0.6490 - val_loss: 1.0803 - val_accuracy: 0.6460
    Epoch 20/20
    63/63 [==============================] - 6s 91ms/step - loss: 1.8435 - accuracy: 0.6320 - val_loss: 1.2554 - val_accuracy: 0.6535
```

The validation accuracy of model2 stabilized between **57% and 64%** during training.
This is a little better than model1. This might be due to the fact that model2 has learned the invariant features of the input images. We also do not observe the overfitting issue on model2, and therefore we can conclude that model2 has a better performance than model1.


## ยง4. Data Preprocessing
**In this section, we will add a layer that makes simple transformations to our input data. To explain in more detail, the layer will normalized the RGB values between between 0 and 1, or possibly between -1 and 1 in order to fasten the training process.**

The following code will create a preprocessing layer that handles the normalize and weight process in advance:
```
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Then, we add this preprocessing layer to our model:
```
model3 = tf.keras.models.Sequential([
    preprocessor,
    first_layer,
    second_layer,
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((3, 3)),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(.2)
])

# compile
model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

#train
history = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)
```
```

    Epoch 1/20
    63/63 [==============================] - 7s 87ms/step - loss: 1.8654 - accuracy: 0.6360 - val_loss: 0.9158 - val_accuracy: 0.7389
    Epoch 2/20
    63/63 [==============================] - 6s 88ms/step - loss: 1.7988 - accuracy: 0.6510 - val_loss: 1.0787 - val_accuracy: 0.6572
    Epoch 3/20
    63/63 [==============================] - 6s 89ms/step - loss: 1.7661 - accuracy: 0.6435 - val_loss: 0.9781 - val_accuracy: 0.7116
    Epoch 4/20
    63/63 [==============================] - 6s 89ms/step - loss: 1.7243 - accuracy: 0.6610 - val_loss: 0.7462 - val_accuracy: 0.7302
    Epoch 5/20
    63/63 [==============================] - 7s 107ms/step - loss: 1.6692 - accuracy: 0.6570 - val_loss: 0.7239 - val_accuracy: 0.7438
    Epoch 6/20
    63/63 [==============================] - 6s 89ms/step - loss: 1.7838 - accuracy: 0.6465 - val_loss: 0.8827 - val_accuracy: 0.7277
    Epoch 7/20
    63/63 [==============================] - 6s 87ms/step - loss: 1.7341 - accuracy: 0.6605 - val_loss: 0.8974 - val_accuracy: 0.7314
    Epoch 8/20
    63/63 [==============================] - 6s 87ms/step - loss: 1.6275 - accuracy: 0.6670 - val_loss: 0.9885 - val_accuracy: 0.6522
    Epoch 9/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.6700 - accuracy: 0.6725 - val_loss: 0.9648 - val_accuracy: 0.7030
    Epoch 10/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.6543 - accuracy: 0.6780 - val_loss: 1.0154 - val_accuracy: 0.7302
    Epoch 11/20
    63/63 [==============================] - 6s 96ms/step - loss: 1.6605 - accuracy: 0.6625 - val_loss: 0.9879 - val_accuracy: 0.7166
    Epoch 12/20
    63/63 [==============================] - 6s 84ms/step - loss: 1.6654 - accuracy: 0.6645 - val_loss: 0.9299 - val_accuracy: 0.7364
    Epoch 13/20
    63/63 [==============================] - 6s 85ms/step - loss: 1.6880 - accuracy: 0.6590 - val_loss: 1.0783 - val_accuracy: 0.7302
    Epoch 14/20
    63/63 [==============================] - 6s 84ms/step - loss: 1.7671 - accuracy: 0.6475 - val_loss: 0.9498 - val_accuracy: 0.7092
    Epoch 15/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.7753 - accuracy: 0.6525 - val_loss: 0.8472 - val_accuracy: 0.7426
    Epoch 16/20
    63/63 [==============================] - 6s 86ms/step - loss: 1.7296 - accuracy: 0.6705 - val_loss: 0.9030 - val_accuracy: 0.7228
    Epoch 17/20
    63/63 [==============================] - 6s 85ms/step - loss: 1.7447 - accuracy: 0.6675 - val_loss: 1.0725 - val_accuracy: 0.7215
    Epoch 18/20
    63/63 [==============================] - 6s 85ms/step - loss: 1.7069 - accuracy: 0.6585 - val_loss: 0.8790 - val_accuracy: 0.7537
    Epoch 19/20
    63/63 [==============================] - 6s 85ms/step - loss: 1.6752 - accuracy: 0.6815 - val_loss: 0.9313 - val_accuracy: 0.7376
    Epoch 20/20
    63/63 [==============================] - 6s 85ms/step - loss: 1.6933 - accuracy: 0.6695 - val_loss: 0.9301 - val_accuracy: 0.7500
```

The validation accuracy of model3 stabilized between **70% and 75%** during training.
There is a huge improvement comparing model3 to model1. By normalizing the RGB values and reweighting them, the model will be able to train faster and more accurately.
We also do not observe overfitting in model3, and this is because model3 is a revised version of model2, and we have resolved the overfitting issue on model2.


## ยง5. Transfer Learning
**In this section, we will access a pre-existing "base model" called MobileNetV2 to our model. MobileNetV2 is a model which others have trained already, and it performs similar task as we do. By adding this layer to our model, we do not need to build our model from scratch anymore.**

```
# download MobileNetV2
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])


# build our final model
model4 = tf.keras.models.Sequential([
    preprocessor,
    first_layer,
    second_layer,
    base_model_layer,
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dropout(.2),
    layers.Dense(2, activation='relu')
])

# summary of our model
model4.summary()

# compile
model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train
history = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data=validation_dataset)

```

```
    Model: "sequential_4"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     model_1 (Functional)        (None, 160, 160, 3)       0         
                                                                     
     random_flip (RandomFlip)    multiple                  0         
                                                                     
     random_rotation (RandomRota  multiple                 0         
     tion)                                                           
                                                                     
     model_2 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     conv2d_12 (Conv2D)          (None, 3, 3, 32)          368672    
                                                                     
     max_pooling2d_12 (MaxPoolin  (None, 1, 1, 32)         0         
     g2D)                                                            
                                                                     
     flatten_4 (Flatten)         (None, 32)                0         
                                                                     
     dropout_4 (Dropout)         (None, 32)                0         
                                                                     
     dense_4 (Dense)             (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 2,626,722
    Trainable params: 368,738
    Non-trainable params: 2,257,984
    _________________________________________________________________

```

```
Epoch 1/20
    63/63 [==============================] - 12s 123ms/step - loss: 0.5845 - accuracy: 0.7285 - val_loss: 0.4313 - val_accuracy: 0.8886
    Epoch 2/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.5538 - accuracy: 0.7685 - val_loss: 0.4487 - val_accuracy: 0.8502
    Epoch 3/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.4624 - accuracy: 0.7675 - val_loss: 0.1089 - val_accuracy: 0.9270
    Epoch 4/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.3533 - accuracy: 0.8300 - val_loss: 0.0847 - val_accuracy: 0.9703
    Epoch 5/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.2986 - accuracy: 0.8375 - val_loss: 0.1139 - val_accuracy: 0.9629
    Epoch 6/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.3756 - accuracy: 0.8070 - val_loss: 0.1033 - val_accuracy: 0.9455
    Epoch 7/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.3624 - accuracy: 0.8370 - val_loss: 0.0805 - val_accuracy: 0.9629
    Epoch 8/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.2585 - accuracy: 0.8575 - val_loss: 0.0698 - val_accuracy: 0.9678
    Epoch 9/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.2291 - accuracy: 0.8890 - val_loss: 0.0655 - val_accuracy: 0.9728
    Epoch 10/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.2129 - accuracy: 0.9005 - val_loss: 0.0734 - val_accuracy: 0.9703
    Epoch 11/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.2136 - accuracy: 0.9090 - val_loss: 0.0612 - val_accuracy: 0.9691
    Epoch 12/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1964 - accuracy: 0.9065 - val_loss: 0.0619 - val_accuracy: 0.9740
    Epoch 13/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.2000 - accuracy: 0.9030 - val_loss: 0.0785 - val_accuracy: 0.9641
    Epoch 14/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1718 - accuracy: 0.9170 - val_loss: 0.0641 - val_accuracy: 0.9715
    Epoch 15/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.1824 - accuracy: 0.9185 - val_loss: 0.0687 - val_accuracy: 0.9703
    Epoch 16/20
    63/63 [==============================] - 6s 93ms/step - loss: 0.1508 - accuracy: 0.9300 - val_loss: 0.0581 - val_accuracy: 0.9777
    Epoch 17/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1575 - accuracy: 0.9325 - val_loss: 0.0660 - val_accuracy: 0.9678
    Epoch 18/20
    63/63 [==============================] - 6s 94ms/step - loss: 0.1730 - accuracy: 0.9220 - val_loss: 0.0590 - val_accuracy: 0.9703
    Epoch 19/20
    63/63 [==============================] - 6s 95ms/step - loss: 0.1580 - accuracy: 0.9265 - val_loss: 0.0548 - val_accuracy: 0.9728
    Epoch 20/20
    63/63 [==============================] - 6s 96ms/step - loss: 0.1804 - accuracy: 0.9200 - val_loss: 0.0603 - val_accuracy: 0.9740
```

The validation accuracy of model4 stabilized between **95% and 97%** during training.
The accuracy score is very high comparing to other models we created from scratch. However, from the model summary, we see that there are over 2 million parameters in our final model, which implies how complex a model needs to be in order to reach 95% accuracy. There is no overfitting in model4, as the overfitting issue has been resolved in model2.

## ยง6. Score on Test Data
**Now we have finished building our model, let's evaluate the accuracy of our model model on the unseen test_dataset.**

```
# predict test data based on model4
test_pred = model4.predict(test_dataset.take(6)).argmax(axis = 1)

# visualize some test data
plt.figure(figsize=(10,10))
for images, labels in test_dataset.take(6):
  for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(images[i].numpy().astype("uint8"))
    
    # label correct if the model correctly classify dogs and cats
    if test_pred[i] == labels[i]:
      plt.xlabel("correct")

plt.show()
```
![post5.4.png](/images/post5.4.png)

From the result, we see that our model perfectly classifies all the 25 random images!

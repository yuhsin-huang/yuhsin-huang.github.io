---
layout: post
title: Blog Post 4 - Spectral Clustering
---

In this blog post, I will create a simple *spectral clustering* algorithm to cluster some crescent data points. 

Spectral clustering is an important tool for assigning unlabeled data to groups according to the distance between each node in a graph.
This clustering technique uses information from the eigenvalues of the (normalized) Laplacian matrix of the similarity matrix built from the data set. I will show how to construct these matrices step by step and use the matrices to build an algorithm that assigns our data to clusters.

## §0. Setup

**We first randomly create a dataset for demonstration.**
```
# import
import numpy as np
from sklearn import dataset
from matplotlib import pyplot as plt

np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
As we can see, this dataset creates two crescents in graph. Each cresent represents one cluster, and our goal is to construct a spectral clustering algorithm that assigns these data correctly.<br><br>
Note that the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`.

## §1. Similarity Matrix
**To start building our spectral clustering algorithm, we construct the *similarity matrix* $\mathbf{A}$ that contains information about the distance of each data point.**

First, set a parameter `epsilon` = 0.4 for now.
Then, the rules for constructing $\mathbf{A}$ are as follow.
- $\mathbf{A}$ is a 2d matrix with shape `(n, n)`. 
- `A[i,j]` = `1` if the distance between `X[i]` and `X[j]` < `epsilon`, otherwise, `A[i,j]` = `0`. 
- The diagonal entries `A[i,i]` should all be equal to zero.

**Here is how I set up $\mathbf{A}$:**
```
from sklearn.metrics import pairwise_distances

epsilon = 0.4

# find pairwise distance between X[i] and X[j]
A = pairwise_distances(X)

# convert A into a matrix
A = np.array(A) 

# if A[i,j]<epsilon, return True, otherwirse, return False
# Then convert boolean to int
A = (A < epsilon) * 1

# fill diagonal with 0's
np.fill_diagonal(A, 0)
```

## §2. Binary Norm Cut Objective of the Similarity Matrix

**With the information of whether each points are near each other, we now start clustering the data points in `X` by getting the `binary norm cut objective`.** 

First, we introduce some notations and assumptions:<br>
- $d_i = \sum_{j = 1}^n a_{ij}$ is the sum of the $i$th row of $\mathbf{A}$, which is also called the *degree* of $i$. 
- $C_0$ and $C_1$ represent two clusters of the data points, in our example, each one of them represents one crescent in the graph. 
- We assume that every data point is in either $C_0$ or $C_1$, where their labels are specified by `y`. 



The *binary norm cut objective* of a matrix $\mathbf{A}$ is then the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

To calculate this, we need the following formulae:
1. $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$, which represents the number of nonzero entries in $\mathbf{A}$ such that one point is in cluster $C_0$ while the other point is in cluster $C_1$. 
2.  $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$.


**Based on the formulae, we first define the cut function and the volume function, then use the returned value from these two functions to get our binary norm cut objective.**


#### The Cut Function
```
def cut(A, y):
    result = 0
    # if i is in C_0 and j is in C_1, add the value of A[i,j] to result
    for i in range(n):
        for j in range(n):
            if y[i]==0 and y[j]==1:
                result+=A[i,j]
    return result
```

#### The Volume Function
```
def vols(A, y):
    
    # compute sum of each row 
    A_row_sum = np.sum(A, axis=1)
    
    # first use np.where to find a list of indices such that y[index]==0 and y[index]==1
    # then sum up the corresponding indices of A_row_sum to get v0 and v1
    v0 = A_row_sum[np.where(y == 0)].sum()
    v1 = A_row_sum[np.where(y == 1)].sum()
    
    return v0, v1
```

#### The Normcut Function
```
def normcut(A, y):
    return cut(A, y) * (1/(vols(A, y)[0]) + 1/(vols(A, y)[1]))
```

## §3. Another Approach to Clustering (Mathematically Friendly)
**From part 2, we know that one approach to clustering is to try to minimize `normcut(A,y)`. In this section, we are going to introduce another mathematically friendly approach.**

First, define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


- Since `vol()` is non-negative, we have that if $i$ is in cluster $C_0$, then $z_i > 0$, and if $i$ is in cluster $C_1$, then $z_i < 0$. 

**Here is how I construct $\mathbf{z}$ :**

```
def transform(A,y):
    # z is a 2-D array with shape = (n,1)
    z = np.ndarray(shape=(n,1))
    
    # use np.where to find a list of indices such that y[index]==0 and y[index]==1
    # (vols(A, y)[0] = vol(C_0) and (vols(A, y)[1] = vol(C_1)
    z[np.where(y == 0)] = 1/(vols(A, y)[0])
    z[np.where(y == 1)] = -1/(vols(A, y)[1])
    
    return z
    
z = transform(A,y)
```


**Next, we show the following equation to prove that $\mathbf{z}$ is a valid approach that is mathematically related to binary norm cut objective:**
$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;$$ 

```
# D is a diagonal matrix with diagonal = row sum of A
D = np.zeros((n,n))
np.fill_diagonal(D, np.sum(A, axis=1))

RHS=(z.T@(D-A)@z)/(z.T@D@z)

# we use np.isclose() instead of "==" because of the rounding issue
np.isclose(normcut(A, y),RHS)
```

The result turns out to be "True".

**Lastly, we check whether $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is `np.ones(n)`. This identity is the premise of $\mathbf{z}$ to be true.**

```
I = np.ones(n)
np.isclose(z.T@D@I, 0)
```

The result is again "True"!

## §4. Optimize $\mathbf{z}$
**In this section, we are going to use the `minimize` function from `scipy.optimize` to minimize $\mathbf{z}$.**

**First, write a function that handles all the conditions required for $\mathbf{z}$:**

```
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

**Then, import `minimize` and get `z_min`.**
```
from scipy.optimize import minimize

# guess is an array with size (n,1) that contained random numbers from -1 to 1
guess = np.random.randint(-1,2, size=(n,1))

res = minimize(orth_obj, guess)
z_min = res.x
```


## §5. Plot (z_min)
**Now we have an array `z_min` that contains information about the cluster label of data points by looking at the signs of its entries. Let's try to plot our data and cluster the data points based on the signs of each entry of z_min.**

```
plt.scatter(X[:,0], X[:,1], c = np.sign(z_min))
```

## §6. Spectral Clustering Using Eigenvalues and Eigenvectors
**In this section, we are going to utilize eigenvalues and eigenvectors to solve the task in part 4.**

*According to the Rayleigh-Ritz Theorem, the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem.*

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Since the corresponding eigenvector of the smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$ is always $\mathbb{1}$, what we want to get for the vector $\mathbf{z}$ is the the eigenvector with the *second*-smallest eigenvalue. 

**To find the eigenvalue, we are going to construct the matrix $\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $\mathbf{A}$.**

```
L = np.linalg.inv(D)@(D-A)
```

**Next, we find the *second*-smallest eigenvalue and plot our data sets based on the signs of z_eig.**

```
from scipy import sparse

# get two sets of eigenvalues and eigenvectors, starting from the smallest eigenvalue
eigval, z_eig = sparse.linalg.eigsh(L, 2, which='SA')

# z_eig[:, 1] represents the second entry of z_eig, which is the corresponding eigenvector of the second smallest eigenvalue
plt.scatter(X[:,0], X[:,1], c = np.sign(z_eig[:, 1]))
```
The result looks good! The plot shows a more accurate clustering of the data comparing to part 5. Only a few data points that are nearer to the cut is miscatagorized. This is reasonable since $\mathbf{A}$ is constructed based on the distance between each data point, and if the distance of data points from two partitions are less than epsilon we set, the data points may be miscatagorized.

## §7. Synthesize Results
**In this part, we are going to combine what we did so far into a function called `spectral_clustering(X, epsilon)`.**

```
def spectral_clustering(X, epsilon):
    ```
    this function conduct spectral clustering to the data set X
    
    Parameters:
    X: Our data set, a matrix that contains the Euclidean coordinates of the data points 
    epsilon: A constant that set the threshold when calculating the distance between each data points
    
    Returns:
    np.sign(z_eig[:, 1]): labels of each data points based on the corresponding eigenvector of the second smallest     eigenvalue of the (normalized) Laplacian matrix of the similarity matrix A.
    
    ```
    # construct A that contains information about the distance of each data point.
    # find pairwise distance
    A = pairwise_distances(X)

    # convert A into a matrix
    A = np.array(A) 

    # if A[i,j]<epsilon, return True, otherwirse, return False
    # Then convert boolean to int
    A = (A < epsilon) * 1

    # fill diagonal with 0's
    np.fill_diagonal(A, 0)
    
    
    # construct D, a diagonal matrix with diagonal = row sum of A
    D = np.zeros((n,n))
    np.fill_diagonal(D, np.sum(A, axis=1))
    
    # construct L: the (normalized) Laplacian matrix of the similarity matrix A
    L = np.linalg.inv(D)@(D-A)
    
    # find the eigenvalues and eigenvectors of L
    eigval, z_eig = sparse.linalg.eigsh(L, 2, which='SA')
    
    return np.sign(z_eig[:, 1])
```

Here is the steps of what we have done in the `spectral_clustering(X, epsilon)` function:
1. Construct the similarity matrix A. 
2. Construct the Laplacian matrix L of A using some formulae we introduced above. 
3. Get the eigenvector with second-smallest eigenvalue of L. 
4. Return labels based on the signs of each entry in this eigenvector. 

## §8. Experiments with Different Nose Arguements
**In this section, We will use our function to run some experiments with data sets generated by `make_moons`, and try out different noise arguements. We will also increase our `n` to `1000` to better visualize our clustering algorithm.**

```
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

```
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

```
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

Since `noise` represents the standard deviation of Gaussian noise added to the data, the larger the noise is, the more scattered the data is. That means when the noise is too high, data points from the same partition will not stay together, and therefore some data points will be mixed with data points from the different partition. Since our spectral clustering algorithm is based on the distance between each data point, when noise is too high, the algorithm will be less accurate since all the data points are mixed together.

## §9. Spectral Clustering with the Bull's Eye and Find the Optimal Epsilon
**In this section, We will try another data set built by `the bull's eye`, and find the optimal epsilon.**

```
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon))
```

```
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```
After trying different epsilon, epsilon in range [0.32, 0.51] works perfectly. This may be due to how the data set is set up. Having `factor = 0.4` sets the distance between the inner and the outer circle be 0.4. Thus, setting epsilon to be near to 0.4 would perform the best. Since our default epsilon is 0.4, I instead show a graph with epsilon = 0.5 for our second demonstration.

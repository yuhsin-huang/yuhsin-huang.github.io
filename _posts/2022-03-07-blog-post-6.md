---
layout: post
title: Blog Post 6 - Fake News Classification
---
In this blog post, I will perform words embedding and develop a fake news classifier algorithm using Tensorflow.

## §1. Setup
**As usual, we import packages and acquire data first.**

```
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import tensorflow as tf

# for eliminating stopwords from data
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords

# for modeling
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

# for visualize the embedding
from sklearn.decomposition import PCA
import plotly.express as px 
```

**Acquire training data:**
```
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
train_df  = pd.read_csv(train_url)
```

## §2. Make a Dataset
**In this section, we first eliminate stopwords from our data, then transform our imported data into a dataset (for modeling purpose). Next, split our dataset into training data and validation data. Lastly, calculate base rate based on our training data.**

```
def make_dataset(title, text):
"""
delete stopwords from the dataframe, then transform the dataframe into a dataset.

arguements:
title, text: the predictor data of the training dataframe

return:
a dataset with a tuple of dictionaries. The first dictionary is the predictor data (title and text), and the second dictionary is the target data (fake).
"""
  # delete stopwords from title and text
  stop = stopwords.words('english')
  train_df['updated_title'] = train_df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  train_df['updated_text'] = train_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  
  # construct dataset  
  data = tf.data.Dataset.from_tensor_slices(
    (
        # predictor data
        {
            "title" : train_df[["updated_title"]], 
            "text" : train_df[["updated_text"]]
        }, 
        
        # target data
        {
            "fake" : train_df[["fake"]]
        }
    )
  )

  return data
```

**Now, we split 20% of our training dataset to use for validation.**

```
data = make_dataset(train_df['title'], train_df['text'])
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

# batch our dataset in order to increase the speed of training
train = data.take(train_size).batch(100)
val   = data.skip(train_size).take(val_size).batch(100)
```

**Lastly, we calculate the base rate of our training data. Recall that base rate refers to the accuracy of a model that always return the same result. In our case here, we consider a model that classify all imputted news as fake news.**

```
fake_iterator= train.unbatch().map(lambda x, y: y['fake']).as_numpy_iterator()

# count how many fake news are actually in our training data
count = 0
for i in fake_iterator:
  if i == 1:
    count+=1
    
# get the rate of the fake news, which is our base rate
count/train_size
```

As a result, we have that approximately 52% of the data in the training dataset is fake, so the base rate is 52%.

## §3. Create Model
**In this section, we create three TensorFlow models to determine which factors (the article title, the article text, both) can detect fake news most accurately.**

**We start by specifying the two kinds of keras.Input for our models.**
- shape specifies the shape of a single element of data. Since both title and text contain only one string in the dictionaries, the shapes of both of them are (1,) (a tuple of length 1).

- name is just some descriptive name we are going to use later.

- dtype describes the type of our input, in our case, both of them are string

```
# inputs
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

**Next, we create some pipelines for processing text.***
```
# only the top 2000 words will be tracked
size_vocabulary = 2000

# a hidden layer we are going to use in our models
# standarize each sameple to lowercase and remove all punctuations, only consider the top 2000 words, 
# outputs are integer, and each headline will be a vector of length 200
vectorize_layer = TextVectorization(
    standardize='lower_and_strip_punctuation',
    max_tokens=size_vocabulary,
    output_mode='int',
    output_sequence_length=500) 

# adapt the vectorized layer to both title and text
vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

**Now we have done preparing our inputs and layers, we start constructing the models.**

1. Use only the article title as an input

```
# layers
title_features = vectorize_layer(title_input)
# into 3D
title_features = layers.Embedding(size_vocabulary, 3, name = "embedding")(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)

model1 = keras.Model(
    inputs = title_input,
    # 2 implies fake or not fake
    outputs = layers.Dense(2, name = "fake")(title_features)
)

# compile
model1.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

# train
history = model1.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = False)
                    
# visualize result
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![post6.1.png](/images/post6.1.png)

From the plot above, we have that the accuracy score of model1 is around 94%.

2. Use only the article text as an input

```
# layers
text_features = vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, 3, name = "embedding2")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)

model2 = keras.Model(
    inputs = text_input,
    outputs = layers.Dense(2, name = "fake")(text_features)
)

# compile
model2.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metric =['accuracy'])

# train
history = model2.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = False)
          
# visualize the result
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![post6.2.png](/images/post6.2.png)

From the plot above, we have that the accuracy score of model2 is between 98.6% and 99.2%.

3. Use both the article title and the article text as input

```
# concatenate the output of the title pipeline with the output of the text pipeline
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation='relu')(main)

# note that the last Dense layer should have a number of outputs equal to the number of classes in the data
output = layers.Dense(2, name = "fake")(main)

model3 = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)

# compile
model3.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

# train
history = model3.fit(train, 
                    validation_data=val,
                    epochs = 30, 
                    verbose = False)

# visualize the result
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```
![post6.3.png](/images/post6.3.png)

From the plot above, we have that the accuracy score of model3 is between 98.2% and 99.8%.

**As a result, both model2 and model3 have very high accuracy score. Hence, we can conclude that article text and the combination of both article text and article title can best detect fake news.**

## §4. Model Evaluation
**In this section, we are going to use our best model from part 3 to predict our test data.**<br>
*Since both model2 and model3 have very high and similar accuracy score, we are going to just choose model3 for predicting the test data.*

```
# acquire test data
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df  = pd.read_csv(test_url)

# convert it into dataset
test = make_dataset(test_df['title'], test_df['text'])

# use model3 to predict
model3.evaluate(test)
```

**We got 99.66% accuracy score!**

## §5. Embedding Visualization
**In this section, we are going to visualize and observe the embedding that our model learned.**

```
# get the weights from the embedding layer
weights = model3.get_layer('embedding').get_weights()[0]
# get the vocabulary from our data
vocab = vectorize_layer.get_vocabulary()                

# reduce the dimension of the layer down to 2D
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

# visualize the embedding
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 hover_name = "word")

fig.show()
```
{% include blog6.4.html %}
- Note that there are lots of country names on the top left of the figure, for example, Japan, Russia, Spain, and Ukraine. They are placed in the same area because they are all related to international news. Once the news related to one of them is fake, the other ones are more likely to be fake as well.
- Note that activist, legislation, urged, and some other protest-related or political vocabularies are closed to each other. This is because these words are more likely to appear on the same news, and so they are placed in similar area while performing word embeddings.

